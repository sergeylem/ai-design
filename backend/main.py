from fastapi import FastAPI, UploadFile, Form
from fastapi.responses import FileResponse
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from controlnet_aux import MidasDetector
import torch
from PIL import Image
from io import BytesIO
import os
from diffusers import AutoencoderKL
from datetime import datetime

device = "mps"

def try_load_vae(pipe, vae_path: str, vae_file: str = "diffusion_pytorch_model.safetensors"):
    config_path = os.path.join(vae_path, "config.json")
    vae_full_path = os.path.join(vae_path, vae_file)

    if os.path.exists(config_path) and os.path.exists(vae_full_path):
        try:
            print("üîÑ –ü–æ–¥–∫–ª—é—á–∞—é –∫–∞—Å—Ç–æ–º–Ω—ã–π VAE...")
            vae = AutoencoderKL.from_pretrained(
                vae_path,
                weight_name=vae_file,
                torch_dtype=torch.float16
            ).to(pipe.device)
            pipe.vae = vae
            print("‚úÖ VAE —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ VAE:", e)
    else:
        print("‚ö†Ô∏è VAE –∏–ª–∏ config.json –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π VAE")

app = FastAPI()

print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é ControlNet...")
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-depth", # ControlNet –¥–ª—è SD 1.5
    torch_dtype=torch.float16
).to(device)

print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é StableDiffusionXL pipeline...")
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", # –ú–æ–¥–µ–ª—å SD 1.5
    controlnet=controlnet,
    torch_dtype=torch.float16
).to(device)

print("üé® –ó–∞–≥—Ä—É–∂–∞—é LoRA –∏–∑ ./loras/lora_v1.0/lora.safetensors ...")
try:
    pipe.load_lora_weights(
        "./loras/sd_1.5", 
        weight_name="etf",
        adapter_name="default"
    )
except Exception as e:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LoRA:", e)
    
# pipe.set_adapters(["default"], adapter_weights=[0.85]) # by default 1, maximum 1.2
# print("‚úÖ LoRA –ø–æ–¥–∫–ª—é—á–µ–Ω–∞ —Å –≤–µ—Å–æ–º 0.85")

# ‚úÖ –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å VAE
# try_load_vae(pipe, "./loras/sdxl_vae")

# üü¢ –ò—Å–ø–æ–ª—å–∑—É–µ–º Depth detector –≤–º–µ—Å—Ç–æ Canny
print("üîç –ó–∞–≥—Ä—É–∂–∞—é Depth –¥–µ—Ç–µ–∫—Ç–æ—Ä (Midas)...")
depth_estimator = MidasDetector.from_pretrained("lllyasviel/ControlNet")

@app.get("/")
async def root():
    return {"message": "AI Design API is running!"}

@app.post("/generate")
async def generate(prompt: str = Form(...), file: UploadFile = Form(...)):
    try:
        print("\nüì§ –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
        print("üìù –ü—Ä–æ–º–ø—Ç:", prompt)
        content = await file.read()
        input_image = Image.open(BytesIO(content)).convert("RGB").resize((512, 512)) # SD 1.5 —Ä–∞–±–æ—Ç–∞–µ—Ç —Å 512x512
        processed_image = depth_estimator(input_image)
        # processed_image = input_image

        result = pipe(
            prompt,
            image=processed_image,
            num_inference_steps=50,  # 50
            negative_prompt= "ugly, bad colors, messy room, unrealistic", #"lowres, blurry, ugly, messy, distorted, bad composition", # "ugly, bad colors, messy room, unrealistic",
            guidance_scale=7.5, # 9,
            guidance_rescale=0.7 #0.8
        ).images[0]

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"generated_images/image_{timestamp}.jpg"
        result.save(filename)
        print("‚úÖ –ö–∞—Ä—Ç–∏–Ω–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤:", filename)
        return {"image_path": filename}

    except Exception as e:
        print("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:", e)
        return {"error": str(e)}

@app.get("/images/{image_name}")
async def get_image(image_name: str):
    return FileResponse(path=f"generated_images/{image_name}", media_type="image/jpeg")
